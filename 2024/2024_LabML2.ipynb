{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNy3hlaGpROkDmGxHzrjbCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/GBDA_LabML2_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ75XfgkLL2h"
      },
      "outputs": [],
      "source": [
        "# Download Indian Pines data, unzip\n",
        "!gdown https://drive.google.com/uc?id=1XxBBah4J3wmSAMFq8lBFc06vGWFiy1TZ\n",
        "!unzip GBDA2020_ML1.zip0.90"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the root directory where the data is located\n",
        "DATA_ROOT = \"partB/\"\n",
        "\n",
        "# Define the class names corresponding to the dataset\n",
        "CLASS_NAMES = [\n",
        "    \"Alfalfa\",\n",
        "    \"Corn-notill\",\n",
        "    \"Corn-mintill\",\n",
        "    \"Corn\",\n",
        "    \"Grass-pasture\",\n",
        "    \"Grass-trees\",\n",
        "    \"Grass-pasture-mown\",\n",
        "    \"Hay-windrowed\",\n",
        "    \"Oats\",\n",
        "    \"Soybean-notill\",\n",
        "    \"Soybean-mintill\",\n",
        "    \"Soybean-clean\",\n",
        "    \"Wheat\",\n",
        "    \"Woods\",\n",
        "    \"Buildings-Grass-Trees-Drives\",\n",
        "    \"Stone-Steel-Towers\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CUYC4ZIRNBIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # Importing PyTorch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split  # Importing necessary modules from PyTorch\n",
        "from sklearn.preprocessing import StandardScaler  # Importing StandardScaler from scikit-learn for data preprocessing\n",
        "import numpy as np  # Importing numpy for numerical operations\n",
        "import os  # Importing os for operating system related operations\n",
        "from copy import copy  # Importing copy module for creating deep copies of objects\n",
        "\n",
        "# Build a custom PyTorch Dataset-compatible class\n",
        "class IndianPinesDataset(Dataset):\n",
        "    def __init__(self, data_root, transforms=[]):\n",
        "        '''\n",
        "        Constructor method for the IndianPinesDataset class.\n",
        "\n",
        "        Parameters:\n",
        "        - data_root (str): Root directory where the dataset is located.\n",
        "        - transforms (list): List of transformations to be applied to the dataset.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.transforms: list = copy(transforms)\n",
        "        self._build(data_root)\n",
        "\n",
        "    def _build(self, data_root) -> None:\n",
        "        '''\n",
        "        Method to load and preprocess the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        - data_root (str): Root directory where the dataset is located.\n",
        "\n",
        "        Returns:\n",
        "        - None\n",
        "        '''\n",
        "        # Load the dataset\n",
        "        img = np.load(os.path.join(data_root, \"indianpinearray.npy\"))\n",
        "        gt_img = np.load(os.path.join(data_root, \"IPgt.npy\"))\n",
        "\n",
        "        # Create a mask to filter out invalid data points\n",
        "        valid_mask = gt_img > 0\n",
        "\n",
        "        # Preprocess the dataset\n",
        "        self.X = img[valid_mask].reshape(-1, 200).astype(np.float32)\n",
        "        self.y = gt_img[valid_mask].reshape(-1).astype(int) - 1  # Subtract 1 to adjust class labels\n",
        "\n",
        "    def apply_std_scaler(self, indices):\n",
        "        '''\n",
        "        Method to apply StandardScaler to the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        - indices (list): List of indices to select samples for fitting the scaler.\n",
        "\n",
        "        Returns:\n",
        "        - None\n",
        "        '''\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(self.X[np.array(indices)])\n",
        "        self.X = scaler.transform(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Method to retrieve dataset elements.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the dataset element to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - X (numpy.ndarray): Input data.\n",
        "        - y (int): Target label.\n",
        "        '''\n",
        "        X, y = self.X[index], self.y[index]\n",
        "        for T in self.transforms:\n",
        "            X, y = T(X, y)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        '''\n",
        "        Method to retrieve the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - int: Total number of samples in the dataset.\n",
        "        '''\n",
        "        return len(self.X)\n"
      ],
      "metadata": {
        "id": "6MmOL5dcNC16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dataset instance\n",
        "dset = IndianPinesDataset(DATA_ROOT)\n",
        "print(\"Samples in dataset: \", len(dset))\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dset, val_dset = random_split(dset, [int(0.7*len(dset)), len(dset)-int(0.7*len(dset))], generator=torch.Generator().manual_seed(2022))\n",
        "\n",
        "# Print the maximum value for the first sample in the validation set before and after scaling\n",
        "print(\"Max value for the first sample in 'val' (before scaling): \", val_dset[0][0].max())\n",
        "dset.apply_std_scaler(train_dset.indices)\n",
        "print(\"Max value for the first sample in 'val' (after scaling)\\t: \", val_dset[0][0].max())\n",
        "\n",
        "# Initialize dataloaders (batching / tensor-casting / shuffling / etc.)\n",
        "BATCH_SIZE = 64\n",
        "train_dloader = DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dloader = DataLoader(val_dset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Inspect the first batch of samples\n",
        "for s in train_dloader:\n",
        "    X, y = s\n",
        "    print(f\"Sample's X type: {type(X)}, dtype: {X.dtype}, shape: {X.size()}\")\n",
        "    print(f\"Sample's y type: {type(y)}, dtype: {y.dtype}, shape: {y.size()}\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "br7pLCNKNEok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn  # Importing necessary module from PyTorch\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features: int, num_classes: int):\n",
        "        '''\n",
        "        Constructor method for the MLP class.\n",
        "\n",
        "        Parameters:\n",
        "        - in_features (int): Number of input features.\n",
        "        - num_classes (int): Number of classes for classification.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the neural network architecture\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        Forward-pass method.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor.\n",
        "        '''\n",
        "        return self.model(x)\n",
        "\n",
        "# Instantiate the MLP model\n",
        "model = MLP(200, 16)\n",
        "\n",
        "# Compile the model for faster training & inference (new in PyTorch 2+)\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "# Print the output shape of the MLP model\n",
        "print(\"MLP's output shape: \", compiled_model(next(iter(val_dloader))[0]).size())\n"
      ],
      "metadata": {
        "id": "06mM_ltaNGdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam  # Importing Adam optimizer from PyTorch\n",
        "from torch.nn import functional as F  # Importing functional module from PyTorch for loss functions\n",
        "\n",
        "LEARNING_RATE = 1e-4  # Setting the learning rate for optimization\n",
        "NUM_EPOCHS = 100  # Setting the number of epochs for training\n",
        "\n",
        "# Transfer model to GPU\n",
        "compiled_model = compiled_model.cuda()\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = Adam(compiled_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Lists to store training and validation losses, and validation overall accuracy\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_oa = []\n",
        "\n",
        "# Training loop\n",
        "for e in range(NUM_EPOCHS):\n",
        "\n",
        "    # Loop over training samples + train for one epoch\n",
        "    total_loss = 0\n",
        "    compiled_model.train()\n",
        "    for batch_sample in train_dloader:\n",
        "        X = batch_sample[0].cuda()\n",
        "        y = batch_sample[1].cuda()\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Infer with the model\n",
        "        preds = compiled_model(X)\n",
        "\n",
        "        # Compute negative log likelihood loss\n",
        "        loss = F.nll_loss(F.log_softmax(preds, dim=-1), y)\n",
        "\n",
        "        # Back-propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Step optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.detach().cpu()\n",
        "    train_losses.append(float(total_loss)/len(train_dloader))\n",
        "\n",
        "    # Validation step\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    compiled_model.eval()\n",
        "    for batch_sample in val_dloader:\n",
        "        X = batch_sample[0].cuda()\n",
        "        y = batch_sample[1].cuda()\n",
        "\n",
        "        # Infer with the model\n",
        "        with torch.no_grad():\n",
        "            preds = compiled_model(X)\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        loss = F.cross_entropy(preds, y, reduction='mean')\n",
        "\n",
        "        total_loss += loss.detach().cpu()\n",
        "        correct += float((torch.argmax(preds, dim=-1) == y).sum())\n",
        "        total += len(y)\n",
        "\n",
        "    # Append validation loss and overall accuracy\n",
        "    val_losses.append(float(total_loss)/len(val_dloader))\n",
        "    val_oa.append(float(correct/total))\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (e+1) % 10 == 0:\n",
        "        print(\"Epoch \", e+1)\n",
        "        print(\"Total training loss: \", train_losses[-1])\n",
        "        print(\"Total validation loss: \", val_losses[-1])\n",
        "        print(\"Overall accuracy: \", val_oa[-1])\n"
      ],
      "metadata": {
        "id": "9scnf7VqNH1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt  # Importing matplotlib for plotting\n",
        "\n",
        "# Plotting the training and validation losses versus epochs\n",
        "plt.figure()\n",
        "plt.title(\"Loss v epochs\")\n",
        "plt.plot(range(1, NUM_EPOCHS+1), train_losses, '-r', label='Training Loss')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_losses, '-g', label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the validation overall accuracy versus epochs\n",
        "plt.figure()\n",
        "plt.title(\"Validation Overall Accuracy v epochs\")\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_oa, '-g')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Overall Accuracy\")\n"
      ],
      "metadata": {
        "id": "RLP1i0wBNJQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix  # Importing necessary functions from scikit-learn\n",
        "import numpy as np  # Importing numpy for numerical operations\n",
        "\n",
        "# Lists to store predictions and targets\n",
        "predictions_list = []\n",
        "targets_list = []\n",
        "\n",
        "# Evaluation mode for the model\n",
        "compiled_model.eval()\n",
        "\n",
        "# Iterate over validation dataloader\n",
        "for batch_sample in val_dloader:\n",
        "    X = batch_sample[0].cuda()\n",
        "    y = batch_sample[1].numpy()\n",
        "    targets_list.append(y)\n",
        "\n",
        "    # Infer with the model\n",
        "    with torch.no_grad():\n",
        "        preds = compiled_model(X)\n",
        "\n",
        "    # Convert predictions to numpy arrays and append to predictions list\n",
        "    predictions_list.append(torch.argmax(preds, dim=-1).cpu().numpy())\n",
        "\n",
        "# Concatenate predictions and targets\n",
        "predictions = np.concatenate(predictions_list, axis=0)\n",
        "targets = np.concatenate(targets_list, axis=0)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cM = confusion_matrix(targets, predictions)\n",
        "\n",
        "# Normalize confusion matrix to precision metric\n",
        "cm_prec = cM / cM.sum(axis=0)\n",
        "\n",
        "# Normalize confusion matrix to recall metric\n",
        "cm_rec = (cM.T / cM.sum(axis=1)).T\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cM, display_labels=CLASS_NAMES)\n",
        "plt.figure(figsize=(20,20), dpi=100)\n",
        "ax = plt.axes()\n",
        "disp.plot(ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(targets, predictions, target_names=CLASS_NAMES))\n"
      ],
      "metadata": {
        "id": "qv7AB6FINKg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}