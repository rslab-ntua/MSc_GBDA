{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/2023/Lab4_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGy9qEBqKfxx"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/2023/Lab4_vit.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K7hQ9zwKfx1"
      },
      "outputs": [],
      "source": [
        "!wget http://madm.dfki.de/files/sentinel/EuroSATallBands.zip\n",
        "!unzip EuroSATallBands.zip\n",
        "!rm EuroSATallBands.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EINoLw-dKfx5"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!pip install lightning\n",
        "!pip install patchify\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCLjfmtHKfx8"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from glob import glob\n",
        "import os\n",
        "import rasterio\n",
        "from typing import Callable, List\n",
        "import numpy as np\n",
        "from patchify import patchify\n",
        "\n",
        "DATA_ROOT = \"ds/images/remote_sensing/otherDatasets/sentinel_2/tif/\"\n",
        "\n",
        "TransformFun = Callable[[dict], dict]\n",
        "\n",
        "class EuroSAT(Dataset):\n",
        "    def __init__(self, data_root, transforms: List[TransformFun] = []):\n",
        "        super().__init__()\n",
        "        self._build_db(data_root)\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def _build_db(self, data_root) -> None:\n",
        "        sample_urls = sorted(glob(os.path.join(data_root, \"**/*.tif\"), recursive=True))\n",
        "        \n",
        "        def parse_category(url):\n",
        "            return os.path.basename(os.path.dirname(url))\n",
        "        \n",
        "        # Get unique category names in alphabetical order\n",
        "        categories = sorted(list(set([parse_category(url) for url in sample_urls])))\n",
        "        self.categories = {c_name: idx for idx, c_name in enumerate(categories)}\n",
        "        \n",
        "        self.db = []\n",
        "        for s_url in sample_urls:\n",
        "            self.db.append({\n",
        "                \"url\": s_url,\n",
        "                \"category_name\": parse_category(s_url),\n",
        "                \"category_id\": self.categories[parse_category(s_url)]\n",
        "            })\n",
        "    \n",
        "    @property\n",
        "    def num_categories(self):\n",
        "        return(len(self.categories))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        sample =  self.db[index]\n",
        "        \n",
        "        for T in self.transforms:\n",
        "            sample = T(sample)\n",
        "            \n",
        "        return sample\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.db)\n",
        "    \n",
        "        \n",
        "\n",
        "def load_data():\n",
        "    def apply(x:dict) -> dict:\n",
        "        assert \"url\" in x\n",
        "        with rasterio.open(x[\"url\"]) as dataset:\n",
        "            x.update({\"data\": dataset.read()[3:0:-1]})\n",
        "        return x\n",
        "    return apply\n",
        "\n",
        "def normalize(factor=10000):\n",
        "    def apply(x:dict) -> dict:\n",
        "        assert \"data\" in x\n",
        "        x[\"data\"] = x[\"data\"].astype(np.float32) / factor\n",
        "        return x\n",
        "    return apply\n",
        "\n",
        "def patchify_transform(n_patches=4):\n",
        "    def apply(x:dict) -> dict:\n",
        "        assert \"data\" in x\n",
        "        channels, height, width = x[\"data\"].shape\n",
        "        assert height == width\n",
        "        assert height % n_patches == 0\n",
        "        patch_size = height // n_patches\n",
        "        sample = patchify(x[\"data\"], (channels,patch_size,patch_size), step=patch_size)\n",
        "        cut_size = sample.shape[1]\n",
        "        x[\"data\"] = x[\"data\"].reshape(1,cut_size**2,channels*patch_size**2)[0]\n",
        "        return x\n",
        "    return apply\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset = EuroSAT(data_root=DATA_ROOT, transforms=[load_data(), normalize()])\n",
        "itdata = iter(dset)"
      ],
      "metadata": {
        "id": "aEPzoy2_g8r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_patches = 8\n",
        "\n",
        "sample = next(itdata)\n",
        "fig_im, ax_im = plt.subplots()\n",
        "ax_im.imshow(sample[\"data\"].transpose(1,2,0))\n",
        "ax_im.set_axis_off()\n",
        "chw = sample[\"data\"].shape\n",
        "\n",
        "patch_side = chw[-1] // n_patches\n",
        "patches = patchify(sample[\"data\"], (chw[0],patch_side,patch_side), step=patch_side)\n",
        "cut_size = patches.shape[1:3]\n",
        "fig, axes = plt.subplots(*cut_size)\n",
        "for id, ax in enumerate(axes.flat):\n",
        "  (i,j) = np.unravel_index(id, cut_size)\n",
        "  patch = patches[0,i,j,...].transpose(1,2,0)\n",
        "  ax.imshow(patch)\n",
        "  ax.set_axis_off()"
      ],
      "metadata": {
        "id": "bIqlMmWJhDD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLRwjqd6KfyB"
      },
      "outputs": [],
      "source": [
        "# Split train/val/test set\n",
        "dset = EuroSAT(data_root=DATA_ROOT, transforms=[load_data(), normalize(), patchify_transform(8)])\n",
        "\n",
        "train_dset, val_dset, test_dset = random_split(dset, \n",
        "        lengths=[0.7,0.2,0.1]\n",
        "    )\n",
        "\n",
        "train_dloader =DataLoader(train_dset, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_dloader =DataLoader(val_dset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_dloader =DataLoader(test_dset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import lightning.pytorch as pl\n",
        "from torchmetrics import Accuracy, ConfusionMatrix\n",
        "from torchsummary import summary\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len: int, emb_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(seq_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, emb_len, 2) * (-math.log(10000.0) / emb_len))\n",
        "        pe = torch.zeros(1,seq_len, emb_len)\n",
        "        pe[0,:,0::2] = torch.sin(position * div_term)\n",
        "        pe[0,:,1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    #  Implementation based on: BrianPulfer/PapersReimplementations/vit\n",
        "    def __init__(self, dim, n_heads=2):\n",
        "        super(MultiheadSelfAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert dim % n_heads == 0, f\"Can't divide dimension {dim} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(dim / n_heads)\n",
        "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        result = torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
        "        return result\n",
        "\n",
        "class ViTLayer(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(ViTLayer, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MultiheadSelfAttention(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "class ViT(pl.LightningModule):\n",
        "    def __init__(self, input_d: int, out_d: int, n_patches: int = 8, \n",
        "                      hidden_d: int = 8, n_layers: int = 2, \n",
        "                      n_heads: int = 2, mlp_ratio: int = 4, lr=5e-3):\n",
        "      # Super constructor\n",
        "        super(ViT, self).__init__()\n",
        "        \n",
        "        self.lr = lr\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.embedding = nn.Linear(input_d, hidden_d)\n",
        "\n",
        "        # 2) Learnable classifiation token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
        "\n",
        "        # 3) Positional encoding\n",
        "        self.pos_embed = PositionalEncoding(n_patches ** 2 + 1, hidden_d)\n",
        "\n",
        "        # 4) Encoder\n",
        "        self.layers = nn.ModuleList([ViTLayer(hidden_d, n_heads, mlp_ratio) for _ in range(n_layers)])\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(hidden_d, n_heads, \n",
        "        #                                             mlp_ratio*hidden_d, \n",
        "        #                                             batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
        "        self.classifier = nn.Linear(hidden_d, out_d)\n",
        "\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=out_d)\n",
        "\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=out_d)\n",
        "        self.val_confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=out_d)\n",
        "        \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        tokens = self.embedding(x)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
        "        \n",
        "        # Adding positional embedding\n",
        "        out = self.pos_embed(tokens)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        out = self.classifier(out[:,0])\n",
        "        return out\n",
        "      \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        X = batch[\"data\"]\n",
        "        y = batch[\"category_id\"]\n",
        "\n",
        "        logits = self(X)\n",
        "\n",
        "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
        "        self.log(\"loss/train\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
        "\n",
        "        self.train_accuracy(logits, y)\n",
        "        self.log(\"accuracy/train\", self.train_accuracy, prog_bar=True, on_epoch=True, on_step=False)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        X = batch[\"data\"]\n",
        "        y = batch[\"category_id\"]\n",
        "\n",
        "        logits = self(X)\n",
        "\n",
        "        loss = F.nll_loss(torch.log_softmax(logits, dim=-1), y)\n",
        "        self.log(\"loss/val\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
        "\n",
        "        self.val_accuracy(logits, y)\n",
        "        self.log(\"accuracy/val\", self.val_accuracy, prog_bar=True, on_epoch=True, on_step=False)\n",
        "\n",
        "        self.val_confusion_matrix(logits, y)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
      ],
      "metadata": {
        "id": "3-Ni2_uEWWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(seq_len=100, emb_len=300)\n",
        "plt.imshow(pe.pe[0])"
      ],
      "metadata": {
        "id": "_kMHiHMOKnH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(192,10,8)\n",
        "x = torch.randn(3,64,192)\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "HyZ6MHWeMJhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24uK0osLKfyH"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"accuracy/val\", mode=\"max\", patience=5),\n",
        "    ModelCheckpoint(monitor=\"accuracy/val\", mode=\"max\", save_last=True)\n",
        "]\n",
        "\n",
        "model = ViT(input_d=192, out_d=dset.num_categories, n_patches=8,\n",
        "            hidden_d=16, n_heads=2, n_layers=2)\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"cpu\", \n",
        "    devices=1,\n",
        "    max_epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    default_root_dir=\"simple_vit\"\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=train_dloader, val_dataloaders=val_dloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W98XtzD1KfyI"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir simple_vit/lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGezDVcYKfyJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "best_model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "trainer.validate(model, dataloaders=test_dloader)\n",
        "\n",
        "cm = model.val_confusion_matrix.compute().cpu().numpy()\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                               display_labels=dset.categories.keys())\n",
        "plt.figure(figsize=(20,20), dpi=100)\n",
        "ax = plt.axes()\n",
        "\n",
        "disp.plot(ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}