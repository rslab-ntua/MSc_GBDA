{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rslab-ntua/MSc_GBDA/blob/master/2025/GBDA_2025_Lab1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Introduction to Neural Networks: Perceptrons and MLPs\n",
    "#\n",
    "# This notebook covers the fundamentals of neural networks, starting with the Perceptron\n",
    "# and moving to Multi-Layer Perceptrons (MLPs) using PyTorch.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 1: Creating and Visualizing Dummy Data\n",
    "#\n",
    "# First, let's create some linearly separable data to demonstrate the Perceptron\n",
    "\n",
    "\n",
    "# Generate linearly separable data\n",
    "def generate_linearly_separable_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate linearly separable data for binary classification.\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "\n",
    "    Returns:\n",
    "        X: Features\n",
    "        y: Labels (0 or 1)\n",
    "    \"\"\"\n",
    "    # Generate two clusters\n",
    "    X = np.zeros((n_samples, 2))\n",
    "    y = np.zeros(n_samples)\n",
    "\n",
    "    # First cluster (class 0)\n",
    "    X[: n_samples // 2, 0] = np.random.normal(loc=-2, scale=1, size=n_samples // 2)\n",
    "    X[: n_samples // 2, 1] = np.random.normal(loc=-2, scale=1, size=n_samples // 2)\n",
    "    y[: n_samples // 2] = 0\n",
    "\n",
    "    # Second cluster (class 1)\n",
    "    X[n_samples // 2 :, 0] = np.random.normal(loc=2, scale=1, size=n_samples // 2)\n",
    "    X[n_samples // 2 :, 1] = np.random.normal(loc=2, scale=1, size=n_samples // 2)\n",
    "    y[n_samples // 2 :] = 1\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Generate data\n",
    "X_linear, y_linear = generate_linearly_separable_data(200)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    X_linear[y_linear == 0, 0],\n",
    "    X_linear[y_linear == 0, 1],\n",
    "    color=\"blue\",\n",
    "    label=\"Class 0\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.scatter(\n",
    "    X_linear[y_linear == 1, 0],\n",
    "    X_linear[y_linear == 1, 1],\n",
    "    color=\"red\",\n",
    "    label=\"Class 1\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.title(\"Linearly Separable Data\", fontsize=15)\n",
    "plt.xlabel(\"Feature 1\", fontsize=12)\n",
    "plt.ylabel(\"Feature 2\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Now let's create some non-linearly separable data for the MLP\n",
    "def generate_nonlinear_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate non-linearly separable data (XOR-like pattern).\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "\n",
    "    Returns:\n",
    "        X: Features\n",
    "        y: Labels (0 or 1)\n",
    "    \"\"\"\n",
    "    X = np.random.randn(n_samples, 2) * 2\n",
    "    y = np.zeros(n_samples)\n",
    "\n",
    "    # XOR-like pattern: points in quadrants 1 and 3 are class 1\n",
    "    # points in quadrants 2 and 4 are class 0\n",
    "    for i in range(n_samples):\n",
    "        if X[i, 0] * X[i, 1] > 0:  # Quadrants 1 and 3\n",
    "            y[i] = 1\n",
    "        else:  # Quadrants 2 and 4\n",
    "            y[i] = 0\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Generate non-linear data\n",
    "X_nonlinear, y_nonlinear = generate_nonlinear_data(200)\n",
    "\n",
    "# Visualize the non-linear data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    X_nonlinear[y_nonlinear == 0, 0],\n",
    "    X_nonlinear[y_nonlinear == 0, 1],\n",
    "    color=\"blue\",\n",
    "    label=\"Class 0\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.scatter(\n",
    "    X_nonlinear[y_nonlinear == 1, 0],\n",
    "    X_nonlinear[y_nonlinear == 1, 1],\n",
    "    color=\"red\",\n",
    "    label=\"Class 1\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.title(\"Non-Linearly Separable Data (XOR-like Pattern)\", fontsize=15)\n",
    "plt.xlabel(\"Feature 1\", fontsize=12)\n",
    "plt.ylabel(\"Feature 2\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "plt.axvline(x=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 2: Simple Perceptron Implementation in PyTorch\n",
    "#\n",
    "# Now let's implement a simple Perceptron model using PyTorch\n",
    "\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Perceptron model implemented in PyTorch.\n",
    "\n",
    "    The Perceptron is the simplest form of a neural network, consisting of:\n",
    "    - Input layer (features)\n",
    "    - A single neuron with weights and bias\n",
    "    - Activation function (Step function in the original Perceptron)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Perceptron.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__()\n",
    "        # Linear layer with one output (binary classification)\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Perceptron.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after passing through the model\n",
    "        \"\"\"\n",
    "        # Apply linear transformation (wx + b)\n",
    "        x = self.linear(x)\n",
    "        # Apply sigmoid activation to get probability\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_linear_tensor = torch.FloatTensor(X_linear)\n",
    "y_linear_tensor = torch.FloatTensor(y_linear).reshape(-1, 1)\n",
    "\n",
    "# Initialize the Perceptron model\n",
    "perceptron = Perceptron(input_dim=2)\n",
    "print(perceptron)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(perceptron.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = perceptron(X_linear_tensor)\n",
    "    loss = criterion(outputs, y_linear_tensor)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Perceptron Training Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        X: Input features\n",
    "        y: Target labels\n",
    "    \"\"\"\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    contour = plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.colorbar(contour, label='Probability of Class (+)')\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"blue\", label=\"Class 0\", alpha=0.7)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"red\", label=\"Class 1\", alpha=0.7)\n",
    "    plt.title(\"Perceptron Decision Boundary\", fontsize=15)\n",
    "    plt.xlabel(\"Feature 1\", fontsize=12)\n",
    "    plt.ylabel(\"Feature 2\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(perceptron, X_linear, y_linear)\n",
    "\n",
    "# Let's try the Perceptron on the non-linear data\n",
    "X_nonlinear_tensor = torch.FloatTensor(X_nonlinear)\n",
    "y_nonlinear_tensor = torch.FloatTensor(y_nonlinear).reshape(-1, 1)\n",
    "\n",
    "# Initialize a new Perceptron\n",
    "perceptron_nonlinear = Perceptron(input_dim=2)\n",
    "\n",
    "# Train on non-linear data\n",
    "optimizer = optim.SGD(perceptron_nonlinear.parameters(), lr=0.1)\n",
    "losses_nonlinear = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = perceptron_nonlinear(X_nonlinear_tensor)\n",
    "    loss = criterion(outputs, y_nonlinear_tensor)\n",
    "    losses_nonlinear.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot the loss curve for non-linear data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_nonlinear)\n",
    "plt.title(\"Perceptron Training Loss on Non-Linear Data\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision boundary for non-linear data\n",
    "plot_decision_boundary(perceptron_nonlinear, X_nonlinear, y_nonlinear)\n",
    "print(\"Notice how the Perceptron fails to separate the non-linear data correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 3: Multi-Layer Perceptron (MLP) in PyTorch\n",
    "#\n",
    "# Now let's implement an MLP to handle the non-linear data\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) implemented in PyTorch.\n",
    "\n",
    "    The MLP consists of:\n",
    "    - Input layer\n",
    "    - One or more hidden layers with non-linear activations\n",
    "    - Output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        \"\"\"\n",
    "        Initialize the MLP.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_dim: Number of neurons in the hidden layer\n",
    "            output_dim: Number of output neurons (1 for binary classification)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Define the network architecture\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor after passing through the model\n",
    "        \"\"\"\n",
    "        # First hidden layer\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Second hidden layer\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize the MLP model\n",
    "mlp = MLP(input_dim=2, hidden_dim=16)\n",
    "print(mlp)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "mlp_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = mlp(X_nonlinear_tensor)\n",
    "    loss = criterion(outputs, y_nonlinear_tensor)\n",
    "    mlp_losses.append(loss.item())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp_losses)\n",
    "plt.title('MLP Training Loss on Non-Linear Data', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision boundary for the MLP\n",
    "plot_decision_boundary(mlp, X_nonlinear, y_nonlinear)\n",
    "print(\"Notice how the MLP can learn the non-linear decision boundary!\")\n",
    "\n",
    "# Compare the loss curves of Perceptron and MLP on non-linear data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_nonlinear, label='Perceptron')\n",
    "plt.plot(mlp_losses, label='MLP')\n",
    "plt.title('Perceptron vs MLP Training Loss on Non-Linear Data', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Part 4: Measuring Loss Demo for Iris Dataset Classification\n",
    "#\n",
    "# Now let's apply our MLP to a real-world dataset: the Iris dataset\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# For simplicity, let's convert this to a binary classification problem\n",
    "# We'll classify setosa (class 0) vs non-setosa (classes 1 and 2)\n",
    "y_binary = (y > 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "# Visualize the Iris dataset (first two features)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "            color='blue', label='Setosa', alpha=0.7)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "            color='red', label='Non-Setosa', alpha=0.7)\n",
    "plt.title('Iris Dataset: Setosa vs Non-Setosa (First Two Features)', fontsize=15)\n",
    "plt.xlabel('Sepal Length (Standardized)', fontsize=12)\n",
    "plt.ylabel('Sepal Width (Standardized)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create an MLP for the Iris dataset\n",
    "iris_mlp = MLP(input_dim=4, hidden_dim=10)\n",
    "print(iris_mlp)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(iris_mlp.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    iris_mlp.train()\n",
    "    train_outputs = iris_mlp(X_train_tensor)\n",
    "    train_loss = criterion(train_outputs, y_train_tensor)\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_preds = (train_outputs > 0.5).float()\n",
    "    train_acc = (train_preds == y_train_tensor).float().mean()\n",
    "    train_accuracies.append(train_acc.item())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    iris_mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = iris_mlp(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_preds = (val_outputs > 0.5).float()\n",
    "        val_acc = (val_preds == y_test_tensor).float().mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, '\n",
    "              f'Val Loss: {val_loss.item():.4f}, Train Acc: {train_acc.item():.4f}, '\n",
    "              f'Val Acc: {val_acc.item():.4f}')\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss Curves', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Accuracy Curves', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "iris_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = iris_mlp(X_test_tensor)\n",
    "    test_preds = (test_outputs > 0.5).float()\n",
    "    test_acc = (test_preds == y_test_tensor).float().mean()\n",
    "    print(f'Test Accuracy: {test_acc.item():.4f}')\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_preds_np = test_preds.numpy().flatten()\n",
    "y_test_np = y_test_tensor.numpy().flatten()\n",
    "\n",
    "cm = confusion_matrix(y_test_np, test_preds_np)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix', fontsize=15)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks([0.5, 1.5], ['Setosa', 'Non-Setosa'])\n",
    "plt.yticks([0.5, 1.5], ['Setosa', 'Non-Setosa'])\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_np, test_preds_np, target_names=['Setosa', 'Non-Setosa']))\n",
    "\n",
    "# Visualize the decision boundary in 2D (using the first two features)\n",
    "def plot_iris_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for the Iris dataset using the first two features.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        X: Input features (first two features only)\n",
    "        y: Target labels\n",
    "    \"\"\"\n",
    "    # Extract the first two features\n",
    "    X_2d = X[:, :2]\n",
    "\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Create a grid with all features set to their means except the first two\n",
    "    X_mean = X.mean(axis=0)\n",
    "    grid = np.zeros((xx.ravel().shape[0], X.shape[1]))\n",
    "    grid[:, 0] = xx.ravel()\n",
    "    grid[:, 1] = yy.ravel()\n",
    "    for i in range(2, X.shape[1]):\n",
    "        grid[:, i] = X_mean[i]\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(torch.FloatTensor(grid))\n",
    "    Z = Z.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    contour = plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "    plt.colorbar(contour, label='Probability of Class 1')\n",
    "    plt.scatter(X_2d[y == 0, 0], X_2d[y == 0, 1], color='blue', label='Setosa', alpha=0.7)\n",
    "    plt.scatter(X_2d[y == 1, 0], X_2d[y == 1, 1], color='red', label='Non-Setosa', alpha=0.7)\n",
    "    plt.title('Decision Boundary (First Two Features)', fontsize=15)\n",
    "    plt.xlabel('Sepal Length (Standardized)', fontsize=12)\n",
    "    plt.ylabel('Sepal Width (Standardized)', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundary for the Iris dataset\n",
    "plot_iris_decision_boundary(iris_mlp, X_test, y_test)\n",
    "\n",
    "# ## Conclusion\n",
    "#\n",
    "# In this notebook, we've covered:\n",
    "# 1. Creating and visualizing dummy data for classification\n",
    "# 2. Implementing a simple Perceptron in PyTorch\n",
    "# 3. Building a Multi-Layer Perceptron (MLP) for non-linear data\n",
    "# 4. Applying an MLP to the Iris dataset and measuring performance\n",
    "#\n",
    "# Key takeaways:\n",
    "# - Perceptrons can only learn linear decision boundaries\n",
    "# - MLPs with hidden layers can learn complex, non-linear decision boundaries\n",
    "# - PyTorch provides a flexible framework for building and training neural networks\n",
    "# - Proper evaluation using train/test splits and metrics like accuracy and confusion matrices is essential"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
